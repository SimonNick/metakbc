./bin/meta-cli.py --train data/wn18rr/train.tsv --dev data/wn18rr/dev.tsv --test data/wn18rr/test.tsv -m complex -k 4000 -b 1000 -e 100 --F2 0.05 --N3 0 -l 0.1 -I reciprocal -V 3 -o adagrad -q
{'F2': 0.05,
 'N3': 0.0,
 'batch_size': 1000,
 'dev': 'data/wn18rr/dev.tsv',
 'embedding_size': 4000,
 'epochs': 100,
 'eval_batch_size': None,
 'input_type': 'reciprocal',
 'learning_rate': 0.1,
 'load': None,
 'model': 'complex',
 'optimizer': 'adagrad',
 'quiet': True,
 'save': None,
 'seed': 0,
 'test': 'data/wn18rr/test.tsv',
 'test_i': None,
 'test_ii': None,
 'train': 'data/wn18rr/train.tsv',
 'validate_every': 3}
INFO:meta-cli.py:Device: cuda
INFO:meta-cli.py:Model state:
INFO:meta-cli.py:	entities.weight	torch.Size([40943, 8000])
INFO:meta-cli.py:	predicates.weight	torch.Size([22, 8000])
Traceback (most recent call last):
  File "./bin/meta-cli.py", line 254, in <module>
    main(sys.argv[1:])
  File "./bin/meta-cli.py", line 216, in main
    loss.backward()
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/torch/tensor.py", line 107, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pminervi/anaconda3/envs/gpu/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 10.73 GiB total capacity; 6.66 GiB already allocated; 1.17 GiB free; 2.13 GiB cached)
